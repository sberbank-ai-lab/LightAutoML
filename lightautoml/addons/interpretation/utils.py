from typing import List, Tuple, Union, Any, Optional, Dict


from collections import defaultdict
import itertools
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import Colormap
import torch


T_untokenized = Union[List[str], Tuple[List[str], List[Any]]]


class WrappedVocabulary:
    """Look-up table (word to token-id), handling unrecognized words.
    
    Args:
        word_to_id: Word to token id dictionary.
        unk_token: Token with which to replace unrecognized words.
    
    """
    def __init__(self,
                 word_to_id: Dict[str, int],
                 unk_token: str = '<UNK>'
                ):
        self.word_to_id = word_to_id
        self.unk_token = word_to_id[unk_token]
        
    def __call__(self, x: str) -> int:
        return self.word_to_id.get(x, self.unk_token)
    
    def __getitem__(self, val: str) -> int:
        return self.word_to_id.get(val, self.unk_token)
    
    def __len__(self):
        return len(self.word_to_id)
    
class WrappedTokenizer:
    """Handler for automl tokenizer.
    
    Args:
        tokenizer: Automl tokenizer.
        
    """
    
    def __init__(self,
                 tokenizer: 'lightautoml.text.tokenizer.BaseTokenizer'
                ):
        self._tokenizer = tokenizer
        
    def __call__(self, x: str) -> List[str]:
        return self._tokenizer.tokenize_sentence(self._tokenizer._tokenize(x))
    

def untokenize(raw: str,
               tokens: List[str],
               return_mask: bool = False,
               token_sym: Any = True,
               untoken_sym: Any = False
              ) -> T_untokenized:
    """Get between tokens symbols.
    
    Args:
        raw: Raw string.
        tokens: List of tokens from raw string.
        return_mask: Flag to return mask 
            for each new token. Format: list of
            `token_sym`, `untoken_sym`.
        token_sym: Object, denote token symbol.
        untoken_sym: Object, denote untoken symbol.
    
    Returns:
        Tuple (full_tokens, tokens_mask) if `return_mask=True`,
            else just list full_tokens.
        
    """
    mask = []
    untokenized = []
    pos = raw.find(tokens[0])
    
    if pos != 0:
        untokenized.append(raw[:pos])
        mask.append(untoken_sym)
        raw = raw[pos:]
    
    prev_token = tokens[0]
    for token in tokens[1:]:
        raw = raw[len(prev_token):]
        pos = raw.find(token)
        untokenized.append(prev_token)
        mask.append(token_sym)
        if pos:
            mask.append(untoken_sym)
            untokenized.append(raw[:pos])
        prev_token = token
        raw = raw[pos:]
        
    untokenized.append(prev_token)
    mask.append(token_sym)
    
    cur = len(prev_token)
    if cur != len(raw):
        untokenized.append(raw[cur:])
        mask.append(untoken_sym)
    
    if return_mask:
        return untokenized, mask
    
    return untokenized


def find_positions(arr: List[str],
                   mask: List[bool]
                  ) -> List[int]:
    """Set positions and tokens.
    
    Args:
        tokens: List of tokens and untokens.
        mask: Mask for tokens. 
        
    Returns:
        List of positions of tokens.
        
    """
    pos = []
    for i, (token, istoken) in enumerate(zip(arr, mask)):
        if istoken:
            pos.append(i)
            
    return pos


class IndexedString:
    """Indexed string."""
    
    def __init__(self,
                 raw_string: str,
                 tokenizer: Any,
                 force_order: bool = True
                ):
        """
        Args:
            raw_string: Raw string.
            tokenizer: Tokenizer class.
            force_order: Save order, or use features as
                bag-of-words. 
        
        """
        self.raw = raw_string
        self.tokenizer = tokenizer
        self.force_order = force_order
        
        self.toks_ = self._tokenize(raw_string)
        self.toks = [token.lower() for token in self.toks_]
        self.as_list_, self.mask = untokenize(
            self.raw, self.toks_, return_mask=True)
        
        self.pos = find_positions(self.as_list_, self.mask)
        self.as_np_ = np.array(self.as_list_)
        self.inv = []
        if not force_order:
            pos = defaultdict(list)
            self.vocab = {}
            for token, cur in zip(self.toks, self.pos):
                if token not in self.vocab:
                    self.vocab[token] = len(self.vocab)
                    self.inv.append(token)
                idx = self.vocab[token]
                pos[idx].append(cur)
            self.pos = pos
        else:
            self.inv = self.toks_
        
        
    def _tokenize(self, text: str) -> List[str]:
        prep_text = self.tokenizer._tokenize(text)
        tokens = self.tokenizer.tokenize_sentence(prep_text)
        
        return tokens
    
    def word(self, idx: int) -> str:
        """Token by its index.
        
        Args:
            idx: Index of token.
            
        Returns:
            Token.
            
        """
        return self.inv[idx]
    
    def inverse_removing(self,
                         to_del: Union[List[str], List[int]],
                         by_tokens: bool = False
                        ) -> str:
        """Remove tokens.
        
        Args:
            to_del: Tokens (text of int) to del.
            by_tokens: Flag if tokens are text or indexes.
            
        Returns:
            String without removed tokens.
        
        """
        
        
        # todo: this type of mapping will be not use order,
        # in case when we have not unique tokens.
        assert (not self.force_order) or \
            (self.force_order and not by_tokens)
        
        if not self.force_order:
            if by_tokens:
                to_del = [self.t_i[token.lower()] for token in to_del]
                to_del = np.array(to_del)
            to_del = list(itertools.chain.from_iterable(
                [self.pos[i] for i in to_del]))
        else:
            to_del = [self.pos[i] for i in to_del]    
        mask = np.ones_like(self.as_np_, dtype=bool)
        mask[to_del] = False
        new_str = ''.join(self.as_np_[mask])
        
        return new_str
            
    @property
    def n_words(self) -> int:
        """Number of unique words."""
        return len(self.pos)
    
    
def draw_html(tokens_and_weights: List[Tuple[str, float]],
              task_name: str,
              cmap: Any = None,
              grad_line: bool = True,
              grad_positive_label: Optional[str] = None,
              grad_negative_label: Optional[str] = None,
              prediction: Optional[float] = None,
              n_ticks: int = 10
             ) -> str:
    """Get colored text in html format.
    
    For color used gradient from cmap.
    
    Args:
        tokens_and_weights: List of tokens. 
        cmap: ```matplotlib.colors.Colormap``` or single color string like (#FFFFFF).
            By default blue-white-red linear gradient is used.
        positive_label: Positive label text.
        negatvie_label: Negative label text.
        
    Returns:
        HTML like string.
    
    """
    font_style = "font-size:14px;"
    
    token_template = (
        '<span style="background-color: {color_hex};">'
            '{token}'
        '</span>'
    )
    
    ticks_template = (
        '<div style="border-left: 1px solid black; height: 18px; float: left; width: {}%;">'
            '<span style="position: relative; top: 1em; left: -{}em">'
                '{:.1f}'
            '</span>'
        '</div>'
    )
    
    gradient_full = (
        "margin-left: 10%; "
        "margin-right: 10%; "
    )
    
    gradient_styling = (
        "background: linear-gradient(90deg, {} 0%, {} 50%, {} 100%); "
        "border: 1px solid black; "
        "margin-left: {}em; "
        "margin-right: {}em; "
        "float: center; "
    )
    ticks_styling = (
        "margin-left: {}em; "
        "margin-right: {}em; "
    )
    norm_const = max(map(lambda x: abs(x[1]), tokens_and_weights))
    order = int('{:.2e}'.format(norm_const).split('e')[1])
    order_s = 'âœ• {:.0e}'.format(10**order)
    lord = 0.5 * len(order_s) + 1.5 # lenght order
    inorm_const = 1 / norm_const
    if cmap is None:
        cmap = plt.get_cmap('bwr')
    
    def get_color_hex(weight):
        if isinstance(cmap, Colormap):
            if task_name == 'reg':
                rgba = cmap((- weight * inorm_const * 0.25 + 0.5), bytes=True)
            else: 
                rgba = cmap((- weight * inorm_const * 0.25 + 0.5), bytes=True)
                # may be sigmoid for classifications, but hard to understand
                # rgba = cmap(1.0 / (1 + np.exp(weight * inorm_const)), bytes=True)
            return '#%02X%02X%02X' % rgba[:3]
        elif isinstance(cmap, str):
            if weight == 0.0:
                return '#FFFFFF'
            return cmap
    
    if prediction is None:
        prediction = ""
        pred_field = ""
    else:
        if task_name == 'reg':
            prediction = "{:.1e}".format(prediction)
        else:
            prediction = "{:.3f}".format(prediction)
        pred_field = "AutoML's prediction"
        
    if grad_line:
        if task_name == "reg":
            grad_positive_label = "Positive"
            grad_negative_label = "Negative"
        elif task_name == "multiclass":
            grad_positive_label = "Class: " + grad_positive_label
            grad_negative_label = "Other classes"
        elif task_name ==  "binary":
            grad_positive_label = "Class: " + grad_positive_label
            grad_negative_label = "Class: " + grad_negative_label
    else:
        grad_positive_label = ""
        grad_negative_label = ""
    
    tokens_html = [
        token_template.format(token=token, color_hex=get_color_hex(weight))
        for token, weight in tokens_and_weights
    ]
    
    if grad_line:
        between_ticks = [(100 / (n_ticks)) - 5e-2 * 6 / n_ticks if i <= n_ticks - 1 else 0 \
                     for i in range(n_ticks + 1)]
        ticks = np.linspace(-norm_const, norm_const, n_ticks + 1) / (10**(order))
        ticks_chart = ' '.join([ticks_template.format(t, 0.7 + 0.385*(k<0), k) \
                                for t, k in zip(between_ticks, ticks)])
        grad_statement = """
        <p style="text-align: center">
            Class mapping
        </p>
        <div style="{}">
            <div id="grad" style="{}">
                <p style="text-align:left; margin-left: 1%; margin-right: 1%; color: white;">
                    {}
                    <span style="float:right;">
                        {}
                    </span>
                </p>
            </div>

            <div style="{}">
                {}
            </div>

            <div style="float: right; right: 0.75em; top: -3em; position: relative; font-weight: bold;">Scale</div>
            <div style="float: right; right: -2em; top: -2.9em; position: relative; font-weight: bold;">{}</div>

            <div style="float: left; left: -5.5em; top: -4.42em; position: relative;  font-weight: bold;">{}</div>
            <div style="float: left; left: -8.22em; top: -2.9em; position: relative;  font-weight: bold;">{}</div>
        </div>
        """.format(
            gradient_full,
            gradient_styling,
            grad_negative_label,
            grad_positive_label,
            ticks_styling.format(
                lord,
                lord),
            ticks_chart,
            order_s,
            pred_field,
            prediction 
        ).format(
            get_color_hex(-norm_const),
            get_color_hex(0.0),                                
            get_color_hex(norm_const),
            lord,
            lord
        )
    else:
        grad_statement = ""
    
    raw_html = """
    <div>
        <p style="text-align: center">
            Text
        </p>
        <div style="border: 1px solid black;">
            <p style="{}; margin-left: 1%; margin-right: 1%;">{}</p>
        </div>
        {}
    </div>
    """.format(
        font_style,
        ' '.join(tokens_html),
        grad_statement
    )
    
    return raw_html

def cross_entropy_multiple_class(input: torch.FloatTensor,
                                 target: torch.FloatTensor
                                ) -> torch.Tensor:
    return torch.mean(torch.sum(target * -torch.log(clamp_probs(input)), dim=1))