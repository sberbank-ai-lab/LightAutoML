general_params:
  # possible values are list of lists, that describes levels
  # possible values are ['lgb', 'lgb_tuned', 'linear_l2', 'cb', 'cb_tuned']. List will be extended later
  # or 'auto' - configuration will be infered from data
  # ex.[['lgb', 'lgb_tuned', 'linear_l2', 'cb', 'cb_tuned'], ['lgb', 'linear_l2']] is 3 algos on 1 level, 2 algos on 2 level and blender on 3 level
  # to define more than 1 level nested_cv_params.cv should be > 1
  use_algos: 'auto'
  # nested_cv - True/False. If true, check nested_cv params. Use folds-in-folds cross-validation scheme
  # May performs better but take much more time to run (train and inference)
  nested_cv: False
  # skip connections
  skip_conn: True
  # Return prediction from all last layer algos?
  return_all_predictions: False
  # The smallest weight in weighted blender (if lower - the algo will be dropped from final model)
  weighted_blender_max_nonzero_coef: 0.05

reader_params:
  # sample of data to perform analisys
  samples: 100000
  # minimum nan_rate in feature to keep feature
  max_nan_rate: 0.999
  # maximum frequency of top frequent value to keep feature
  max_constant_rate: 0.999
  # create validation folds. Folds will be created even if valid samples or custom validation will be passed
  # it will be used for feature generation
  cv: 5
  # random state for folds creation
  random_state: 42
  # default roles params.
  # Ex if {'numeric': {'force_input': True}} will be passed, all numeric features will pass all selectors,
  # if another will not be passed in roles argument of .fit_predict
  roles_params:
  # n_jobs for advanced roles guess and reading csv files (more RAM needed due to multiprocessing)
  n_jobs: 8
  # If to turn on advanced roles guess. Slower, but shows better quality
  # If role of feature is not defined, reader will guess role in 2 ways:
  #   - simple (numbers as numbers, object as categories or dates if dateformat inferred)
  #   - advanced, based on some statistic calculation
  # Advanced also searches to best processing type for defined roles, and in most cases is better, but slower
  advanced_roles: True
  # advanced roles parsing params
  # defaults are ok in general case, don't touch it if you don't know it's meanings
  numeric_unique_rate: 0.999
  max_to_3rd_rate: 1.1
  binning_enc_rate: 2
  raw_decr_rate: 1.1
  max_score_rate: 0.2
  abs_score_val: 0.04
  drop_score_co: 0.00

read_csv_params:
  # params for pandas.read_csv func
  decimal: '.'
  sep: ','
  # another params from pandas read_csv docs can be added...

nested_cv_params:
  # params describe how to use inner cross validation (folds-in-folds) to get unbiased oof prediction
  # cv defines how many folds we split dataset to perform nested cross validation. 1 means no nested cv
  cv: 5
  # n_folds defines how many cv loops to perform. None for all or int < cv
  n_folds:
  # how to perform params tuning. If True - we tune params on inner cv loops, False - on outer validation
  inner_tune: False
  # should we refit tuner each inner cv loop or just take first
  refit_tuner: True

selection_params:
  # selection mode 0/1/2
  # 0 for no selection, 1 - cutoff selection, 2 - iterative selection
  # harder features selection means increasing train time, but much smaller and faster for inference model
  # 1 is good in most cases
  mode: 1
  # importance type permutation/gain
  importance_type: 'gain'
  # pretrain selector on holdout set. True - fast/ False - accurate
  fit_on_holdout: True
  # cutoff value for permutation or gain (mode 1/2)
  cutoff: 0
  # group features add size for iterative algo (mode 2)
  feature_group_size: 1
  # max features count (mode2)
  max_features_cnt_in_result:
  # list of algos to apply selector. Possible values - 'gbm', 'linear_l2'. gbm stands for both catboost and lgb
  select_algos: [ 'gbm' ]

tuning_params:
  # pretrain tuner on holdout set. True - fast/ False - accurate
  # Ex. if you have 5-fold cv, validate tuner only on 1 fold
  fit_on_holdout: True
  # max tuning iter for lightgbm. Auto - depends on dataset
  # smaller dataset gets more iters (int or 'auto')
  max_tuning_iter: 101
  # max tuning time. Tuning time might be set lower during train by automl's timer, but cannot be higher
  max_tuning_time: 300

# params for BoostLGBM MLAlgo. Note - params are default and may be changed during train if not freeze_defaults
lgb_params:
  # Look for lightgbm train params here. (num_threads will be rewrited by global preset's cpu limit)
  default_params:
    num_threads: 100
  freeze_defaults: False

# params for BoostCB MLAlgo. Note - params are default and may be changed during train if not freeze_defaults
cb_params:
  # Look for lightgbm train params here. (thread_count will be rewrited by global preset's cpu limit)
  default_params:
    # task type - auto based on gpu available
    task_type: 'CPU'
    thread_count: 100
  freeze_defaults: False

# params for LinearLBFGS MLAlgo
# no tuner needed for this algo - regularization params are found during fit
linear_l2_params:
  default_params: { }
  freeze_defaults: False

gbm_pipeline_params:
  # max number of categories to generate intersections
  top_intersections: 4
  # max depth of cat intersection
  max_intersection_depth: 3
  # subsample to calc data statistics
  subsample: 100000
  auto_unique_co: 10
  # n_classes to use target encoding for multiclass task
  multiclass_te_co: 3
  # DEV feature: output categorical features as categories (if True, can totally overfit your model - be careful!)
  output_categories: False

linear_pipeline_params:
  # max number of categories to generate intersections
  top_intersections: 4
  # max depth of cat intersection
  max_intersection_depth: 3
  # subsample to calc data statistics
  subsample: 100000
  auto_unique_co: 50
  # n_classes to use target encoding for multiclass task
  multiclass_te_co: 3

timing_params:
  # select timing mode:
  # 0: no limits - use time limits to create algo's settings but if automl run out of time - let it finish
  # 1: soft - approximate time limits - tasks will finished after timeout
  # 2: hard - hard time limits - stop all tasks before timeout to be exactly in time
  # Any time limitation mode will start working after at least single fold of single model will be computed
  mode: 1
  overhead: 0.1
  # we assume than each algo takes same amount of time to calc (adjusted with some timing score).
  # So each algo gets TIME/N_ALGOS.
  # tuning_rate of that time can be given to the params tuner
  # 0 - means no tuning
  # 'auto' - means infer depends on dataset size
  tuning_rate: 0.7